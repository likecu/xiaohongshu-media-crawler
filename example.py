#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦å¤šå…³é”®è¯çˆ¬è™«ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¤šå…³é”®è¯çˆ¬è™«æ¥çˆ¬å–å°çº¢ä¹¦ä¸Šçš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. åˆå§‹åŒ–çˆ¬è™«
2. ä»é…ç½®æ–‡ä»¶è¯»å–æœç´¢å…³é”®è¯
3. ä»é…ç½®æ–‡ä»¶è¯»å–çˆ¬å–å‚æ•°
4. è¿è¡Œçˆ¬è™«
5. å¤„ç†çˆ¬å–ç»“æœ
6. HTMLç”ŸæˆåŠŸèƒ½
7. å¸–å­æ€»ç»“åŠŸèƒ½
"""

import json
import os
import time
from typing import List, Dict, Any
from xhs_crawler.crawlers.multi_keyword_crawler import MultiKeywordCrawler
from xhs_crawler.generators.generate_complete_html import CompleteHtmlGenerator
from xhs_crawler.generators.generate_html_from_existing import ExistingHtmlGenerator
import subprocess

def print_menu():
    """
    æ‰“å°åŠŸèƒ½èœå•
    """
    print("=" * 60)
    print("ğŸ¯ å°çº¢ä¹¦çˆ¬è™«ä¸åˆ†æå·¥å…·å¥—ä»¶")
    print("=" * 60)
    print("1. ğŸ” è¿è¡Œå¤šå…³é”®è¯çˆ¬è™«")
    print("   - ä»é…ç½®æ–‡ä»¶è¯»å–æœç´¢å…³é”®è¯")
    print("   - çˆ¬å–æŒ‡å®šé¡µæ•°çš„å°çº¢ä¹¦å†…å®¹")
    print("   - ä¿å­˜çˆ¬å–ç»“æœåˆ°æœ¬åœ°")
    print()
    print("2. ğŸ“„ ä»æœç´¢ç»“æœç”Ÿæˆå®Œæ•´HTML")
    print("   - åŠ è½½æœç´¢ç»“æœå’Œå¸–å­è¯¦æƒ…")
    print("   - ç”ŸæˆåŒ…å«å®Œæ•´å¸–å­ä¿¡æ¯çš„HTMLç½‘é¡µ")
    print()
    print("3. ğŸ“‚ ä»ç°æœ‰æ•°æ®ç”ŸæˆHTML")
    print("   - ä»å·²æœ‰çš„æœç´¢ç»“æœæ–‡ä»¶ç”ŸæˆHTML")
    print("   - æ”¯æŒå¤šç§æœç´¢ç»“æœæ–‡ä»¶æ ¼å¼")
    print()
    print("4. ğŸ“ å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“")
    print("   - ä½¿ç”¨gemini_ocr.pyå¯¹å¸–å­å†…å®¹è¿›è¡ŒOCRè¯†åˆ«")
    print("   - å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“")
    print("   - ç”ŸæˆåŒ…å«æ€»ç»“çš„HTMLç½‘é¡µ")
    print()
    print("5. ğŸ”¥ æå–çƒ­é—¨å…³é”®è¯")
    print("   - ä»å°çº¢ä¹¦æœç´¢ç»“æœä¸­æå–çƒ­é—¨å…³é”®è¯")
    print("   - æ ¹æ®å¸–å­çƒ­åº¦è®¡ç®—å…³é”®è¯çƒ­åº¦åˆ†æ•°")
    print("   - ç”Ÿæˆçƒ­é—¨å…³é”®è¯æ’è¡Œæ¦œ")
    print()
    print("0. ğŸšª é€€å‡ºç¨‹åº")
    print("=" * 60)

def run_multi_keyword_crawler():
    """
    è¿è¡Œå¤šå…³é”®è¯çˆ¬è™«
    """
    print("\nğŸ¯ å°çº¢ä¹¦å¤šå…³é”®è¯çˆ¬è™«ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–çˆ¬è™«
    print("\n1. åˆå§‹åŒ–å¤šå…³é”®è¯çˆ¬è™«...")
    crawler = MultiKeywordCrawler()
    print("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    # 2. ä»é…ç½®æ–‡ä»¶è¯»å–æœç´¢å…³é”®è¯å’Œçˆ¬å–å‚æ•°
    print("\n2. ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°...")
    config_file = "search_config.json"
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨")
        return
    
    # è¯»å–é…ç½®æ–‡ä»¶
    with open(config_file, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # è·å–æœç´¢å…³é”®è¯
    keywords: List[str] = config.get("search_terms", ["å¤§æ¨¡å‹é¢è¯•"])
    print(f"ğŸ“ æœç´¢å…³é”®è¯: {', '.join(keywords)}")
    
    # è·å–çˆ¬å–å‚æ•°
    max_pages: int = config.get("page_num", 2)  # æ¯ä¸ªå…³é”®è¯çˆ¬å–çš„æœ€å¤§é¡µæ•°
    page_size: int = config.get("page_size", 10)  # æ¯é¡µè¿”å›çš„å¸–å­æ•°é‡
    print(f"ğŸ“‹ çˆ¬å–é…ç½®: æœ€å¤§é¡µæ•°={max_pages}, æ¯é¡µæ•°é‡={page_size}")
    
    # 4. è¿è¡Œçˆ¬è™«
    print("\n4. å¯åŠ¨çˆ¬å–...")
    try:
        crawler.run(keywords=keywords, max_pages=max_pages, page_size=page_size)
        print("\nâœ… çˆ¬å–å®Œæˆï¼")
        
        # 5. æ˜¾ç¤ºç»“æœä¿¡æ¯
        print("\n5. çˆ¬å–ç»“æœä¿¡æ¯:")
        print(f"   ğŸ“ ç»“æœä¿å­˜ç›®å½•: {crawler.output_dir}")
        print(f"   ğŸŒ HTMLç½‘é¡µ: {crawler.html_file}")
        print(f"   ğŸ’¡ å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹å®Œæ•´ç»“æœ")
        
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€MCPæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ çˆ¬è™«æ¼”ç¤ºç»“æŸ")

def generate_complete_html():
    """
    ä»æœç´¢ç»“æœç”Ÿæˆå®Œæ•´HTML
    """
    print("\nğŸ“„ ä»æœç´¢ç»“æœç”Ÿæˆå®Œæ•´HTML")
    print("=" * 50)
    
    try:
        generator = CompleteHtmlGenerator()
        generator.run()
    except Exception as e:
        print(f"\nâŒ ç”ŸæˆHTMLè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²è¿è¡Œçˆ¬è™«å¹¶ç”Ÿæˆäº†æœç´¢ç»“æœ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ HTMLç”Ÿæˆæ¼”ç¤ºç»“æŸ")

def generate_html_from_existing():
    """
    ä»ç°æœ‰æ•°æ®ç”ŸæˆHTML
    """
    print("\nğŸ“‚ ä»ç°æœ‰æ•°æ®ç”ŸæˆHTML")
    print("=" * 50)
    
    try:
        generator = ExistingHtmlGenerator()
        generator.run()
    except Exception as e:
        print(f"\nâŒ ç”ŸæˆHTMLè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²è¿è¡Œçˆ¬è™«å¹¶ç”Ÿæˆäº†æœç´¢ç»“æœ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ç°æœ‰æ•°æ®HTMLç”Ÿæˆæ¼”ç¤ºç»“æŸ")

def summarize_posts():
    """
    å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“
    """
    print("\nğŸ“ å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“")
    print("=" * 50)
    
    try:
        # è°ƒç”¨summarize_posts.pyè„šæœ¬
        script_path = os.path.join("xhs_crawler", "summarizers", "summarize_posts.py")
        if os.path.exists(script_path):
            print(f"ğŸ” æ‰§è¡Œè„šæœ¬: {script_path}")
            # ä½¿ç”¨cwdå‚æ•°ç¡®ä¿è„šæœ¬åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œ
            result = subprocess.run(
                ["python3", script_path],
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=600  # è®¾ç½®10åˆ†é’Ÿè¶…æ—¶ï¼Œå¤„ç†å¤§é‡å¸–å­éœ€è¦æ›´é•¿æ—¶é—´
            )
            print(result.stdout)
            if result.stderr:
                print(f"\nâŒ è„šæœ¬æ‰§è¡Œé”™è¯¯: {result.stderr}")
        else:
            print(f"âŒ è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_path}")
    except subprocess.TimeoutExpired:
        print("\nâŒ è„šæœ¬æ‰§è¡Œè¶…æ—¶")
        print("ğŸ’¡ æç¤º: å¤„ç†å¤§é‡å¸–å­å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼Œå¯ä»¥å°è¯•å¢åŠ è¶…æ—¶æ—¶é—´")
    except Exception as e:
        print(f"\nâŒ æ€»ç»“è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿gemini_ocr.pyå·¥å…·è·¯å¾„æ­£ç¡®ä¸”å¯æ‰§è¡Œ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ å¸–å­æ€»ç»“æ¼”ç¤ºç»“æŸ")

def extract_hot_keywords():
    """
    æå–çƒ­é—¨å…³é”®è¯
    """
    print("\nğŸ”¥ æå–å°çº¢ä¹¦çƒ­é—¨å…³é”®è¯")
    print("=" * 50)
    try:
        import os
        from xhs_crawler.summarizers.hot_keywords import (
            extract_hot_keywords_from_directory,
            display_hot_keywords,
            save_hot_keywords
        )
        
        # è‡ªåŠ¨æ£€æµ‹çˆ¬è™«è¾“å‡ºç›®å½•
        output_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and 'å¸–å­' in d]
        if output_dirs:
            directory = output_dirs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ç›®å½•
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°çˆ¬è™«è¾“å‡ºç›®å½•: {directory}")
        else:
            directory = "output"  # é»˜è®¤ç›®å½•
            print(f"â„¹ï¸  æœªæ£€æµ‹åˆ°çˆ¬è™«è¾“å‡ºç›®å½•ï¼Œä½¿ç”¨é»˜è®¤ç›®å½•: {directory}")
        
        # æå–çƒ­é—¨å…³é”®è¯ï¼Œå¢åŠ å…³é”®è¯æ•°é‡
        hot_keywords = extract_hot_keywords_from_directory(directory=directory, top_n=50)
        
        if hot_keywords:
            # æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯
            display_hot_keywords(hot_keywords)
            
            # ä¿å­˜çƒ­é—¨å…³é”®è¯
            save_hot_keywords(hot_keywords, "hot_keywords.json")
        else:
            print("æœªæå–åˆ°çƒ­é—¨å…³é”®è¯")
            
    except ImportError as e:
        print(f"\nâŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ¨¡å—è·¯å¾„æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        print(f"\nâŒ æå–çƒ­é—¨å…³é”®è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ çƒ­é—¨å…³é”®è¯æå–æ¼”ç¤ºç»“æŸ")

def main():
    """
    ä¸»å‡½æ•°ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰åŠŸèƒ½
    """
    print("ğŸ¯ å°çº¢ä¹¦çˆ¬è™«ä¸åˆ†æå·¥å…·å¥—ä»¶ - è‡ªåŠ¨æ‰§è¡Œæ¨¡å¼")
    print("=" * 60)
    print("ç¨‹åºå°†æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹åŠŸèƒ½ï¼š")
    print("1. ğŸ” è¿è¡Œå¤šå…³é”®è¯çˆ¬è™«")
    print("2. ğŸ“„ ä»æœç´¢ç»“æœç”Ÿæˆå®Œæ•´HTML")
    print("3. ğŸ“‚ ä»ç°æœ‰æ•°æ®ç”ŸæˆHTML")
    print("4. ğŸ“ å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“")
    print("5. ğŸ”¥ æå–çƒ­é—¨å…³é”®è¯")
    print("=" * 60)
    
    # æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰åŠŸèƒ½
    print("\n\n" + "=" * 60)
    print("å¼€å§‹æ‰§è¡ŒåŠŸèƒ½ 1: ğŸ” è¿è¡Œå¤šå…³é”®è¯çˆ¬è™«")
    print("=" * 60)
    run_multi_keyword_crawler()
    
    print("\n\n" + "=" * 60)
    print("å¼€å§‹æ‰§è¡ŒåŠŸèƒ½ 2: ğŸ“„ ä»æœç´¢ç»“æœç”Ÿæˆå®Œæ•´HTML")
    print("=" * 60)
    generate_complete_html()
    
    print("\n\n" + "=" * 60)
    print("å¼€å§‹æ‰§è¡ŒåŠŸèƒ½ 3: ğŸ“‚ ä»ç°æœ‰æ•°æ®ç”ŸæˆHTML")
    print("=" * 60)
    generate_html_from_existing()
    
    print("\n\n" + "=" * 60)
    print("å¼€å§‹æ‰§è¡ŒåŠŸèƒ½ 4: ğŸ“ å¯¹å¸–å­å†…å®¹è¿›è¡Œæ€»ç»“")
    print("=" * 60)
    summarize_posts()
    
    print("\n\n" + "=" * 60)
    print("å¼€å§‹æ‰§è¡ŒåŠŸèƒ½ 5: ğŸ”¥ æå–çƒ­é—¨å…³é”®è¯")
    print("=" * 60)
    extract_hot_keywords()
    
    print("\n\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½æ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
