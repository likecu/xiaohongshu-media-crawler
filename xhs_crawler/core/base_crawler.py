#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«åŸºç±»ï¼ŒåŒ…å«æ‰€æœ‰çˆ¬è™«çš„å…±åŒé€»è¾‘
"""

import os
import json
from typing import List, Dict, Any, Optional
from xhs_crawler.core.mcp_utils import MCPUtils, ensure_directory, save_json_data, clean_filename
from xhs_crawler.core.ai_utils import AIUtils
from xhs_crawler.generators.html_generator import generate_html


class BaseCrawler:
    """
    çˆ¬è™«åŸºç±»ï¼ŒåŒ…å«æ‰€æœ‰çˆ¬è™«çš„å…±åŒé€»è¾‘
    """
    
    def __init__(self, output_dir: str = "output"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.mcp_utils = MCPUtils()
        self.ai_utils = AIUtils()
        self.output_dir = output_dir
        self.detail_dir = os.path.join(output_dir, "è¯¦æƒ…")
        self.ensure_output_dirs()
    
    def ensure_output_dirs(self) -> None:
        """
        ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        """
        ensure_directory(self.output_dir)
        ensure_directory(self.detail_dir)
    
    def search_posts(self, keywords: str = "å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«", page_num: int = 1, page_size: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢å°çº¢ä¹¦å¸–å­
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            å¸–å­åˆ—è¡¨
        """
        print(f"ğŸ” æœç´¢å…³é”®è¯: '{keywords}'...")
        
        result = self.mcp_utils.call_mcp_tool("xhs_search", {
            "keywords": keywords,
            "page_num": page_num,
            "page_size": page_size
        })
        
        if result.get("code") != 0:
            print(f"âŒ æœç´¢å¤±è´¥: {result.get('msg')}")
            return []
        
        data = result.get("data", {})
        notes = data.get("notes", [])
        print(f"âœ… æ‰¾åˆ° {len(notes)} ç¯‡å¸–å­")
        
        return notes
    
    def get_post_detail(self, note_id: str, xsec_token: str, xsec_source: str = "pc_feed") -> Dict[str, Any]:
        """
        è·å–å¸–å­è¯¦æƒ…
        
        Args:
            note_id: å¸–å­ID
            xsec_token: å®‰å…¨ä»¤ç‰Œ
            xsec_source: æ¥æº
            
        Returns:
            å¸–å­è¯¦æƒ…
        """
        print(f"ğŸ“‹ è·å–å¸–å­è¯¦æƒ…: {note_id}...")
        
        result = self.mcp_utils.call_mcp_tool("xhs_crawler_detail", {
            "note_id": note_id,
            "xsec_token": xsec_token,
            "xsec_source": xsec_source
        })
        
        if result.get("code") != 0:
            print(f"âŒ è·å–å¸–å­è¯¦æƒ…å¤±è´¥: {result.get('msg')}")
            return {}
        
        data = result.get("data", {})
        notes = data.get("notes", [])
        
        if notes:
            return notes[0]
        
        return {}
    
    def get_post_comments(self, note_id: str, xsec_token: str, page_num: int = 1, page_size: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–å¸–å­è¯„è®º
        
        Args:
            note_id: å¸–å­ID
            xsec_token: å®‰å…¨ä»¤ç‰Œ
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        print(f"ğŸ’¬ è·å–å¸–å­è¯„è®º: {note_id}...")
        
        result = self.mcp_utils.call_mcp_tool("xhs_crawler_comments", {
            "note_id": note_id,
            "xsec_token": xsec_token,
            "page_num": page_num,
            "page_size": page_size
        })
        
        if result.get("code") != 0:
            print(f"âŒ è·å–è¯„è®ºå¤±è´¥: {result.get('msg')}")
            return []
        
        return result.get("data", {}).get("comments", [])
    
    def crawl_posts(self, keywords: str = "å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«", page_num: int = 1, page_size: int = 10) -> List[Dict[str, Any]]:
        """
        å®Œæ•´æŠ“å–æµç¨‹
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            åŒ…å«è¯¦æƒ…çš„å¸–å­åˆ—è¡¨
        """
        # 1. æœç´¢å¸–å­
        notes = self.search_posts(keywords, page_num, page_size)
        
        posts = []
        for i, note in enumerate(notes):
            note_id = note.get("note_id")
            if not note_id:
                continue
                
            print(f"\nğŸ“Œ å¤„ç†ç¬¬ {i+1}/{len(notes)} ç¯‡å¸–å­")
            
            # 2. è·å–å¸–å­è¯¦æƒ…
            detail = self.get_post_detail(
                note_id=note_id,
                xsec_token=note.get("xsec_token", ""),
                xsec_source=note.get("xsec_source", "pc_feed")
            )
            
            if detail:
                # 3. ä½¿ç”¨ AI è¿›è¡Œå†…å®¹åˆ†æ
                print(f"ğŸ§  ä½¿ç”¨ AI åˆ†æå†…å®¹...")
                content = detail.get("desc", "")
                title = note.get("title", "")
                images = detail.get("imageList", [])
                
                enhanced_summary = self.ai_utils.summarize_content_enhanced(
                    content=content,
                    title=title,
                    images=images
                )
                
                post = {
                    "basic_info": note,
                    "detail": detail,
                    "enhanced_summary": enhanced_summary
                }
                posts.append(post)
                
                # ä¿å­˜è¯¦æƒ…
                title = note.get("title", f"å¸–å­{i+1}")
                clean_title = clean_filename(title)
                filename = f"{i+1:03d}_{clean_title}_detail.json"
                save_json_data(post, os.path.join(self.detail_dir, filename))
            
            # ä¿å­˜åŸå§‹å¸–å­ä¿¡æ¯
            post_filename = f"{i+1:03d}_{clean_title}.json"
            save_json_data(note, os.path.join(self.output_dir, post_filename))
        
        # 4. æ„å»ºå†…å®¹ç´¢å¼•ç”¨äºç›¸ä¼¼åº¦æœç´¢å’Œæ¨è
        if posts:
            print(f"ğŸ“Š æ„å»ºå†…å®¹ç´¢å¼•...")
            self.ai_utils.build_content_index(posts)
            print(f"âœ… å†…å®¹ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼• {len(posts)} ç¯‡å¸–å­")
            
        return posts
    
    def generate_html_page(self, posts: List[Dict[str, Any]], html_file: str, title: str = "å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«") -> bool:
        """
        ç”ŸæˆHTMLç½‘é¡µ
        
        Args:
            posts: å¸–å­åˆ—è¡¨
            html_file: HTMLæ–‡ä»¶è·¯å¾„
            title: ç½‘é¡µæ ‡é¢˜
            
        Returns:
            æ˜¯å¦ç”ŸæˆæˆåŠŸ
        """
        return generate_html(posts, html_file, title)
    
    def deduplicate_notes(self, notes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹å¸–å­åˆ—è¡¨è¿›è¡Œå»é‡
        
        Args:
            notes: å¸–å­åˆ—è¡¨
            
        Returns:
            å»é‡åçš„å¸–å­åˆ—è¡¨
        """
        seen_note_ids = set()
        unique_notes = []
        
        for note in notes:
            note_id = note.get("note_id")
            if note_id and note_id not in seen_note_ids:
                seen_note_ids.add(note_id)
                unique_notes.append(note)
        
        print(f"âœ… å»é‡å‰: {len(notes)} ç¯‡ï¼Œå»é‡å: {len(unique_notes)} ç¯‡")
        return unique_notes
