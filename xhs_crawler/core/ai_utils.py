#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI/MLå·¥å…·æ¨¡å—ï¼Œæä¾›å¢å¼ºçš„å†…å®¹åˆ†æå’Œæ¨èåŠŸèƒ½
"""

import os
import sys
import json
import re
import time
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from xhs_crawler.core.mcp_utils import MCPUtils
from xhs_crawler.core.database import get_neon_database

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥é…ç½®
from xhs_crawler.core.config import get_output_dir, get_detail_dir

# OCRå·¥å…·è·¯å¾„ï¼ˆä»é…ç½®è·å–ï¼‰
from xhs_crawler.core.config import OCR_CONFIG
OCR_TOOL = OCR_CONFIG["tool_path"]


class AIUtils:
    """
    AI/MLå·¥å…·ç±»ï¼Œæä¾›å¢å¼ºçš„å†…å®¹åˆ†æå’Œæ¨èåŠŸèƒ½
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–AIå·¥å…·
        """
        self.mcp_utils = MCPUtils()
        self.tfidf_vectorizer = TfidfVectorizer(stop_words=None)
        self.post_vectors = None
        self.posts = None
    
    def summarize_content_enhanced(self, content: str, title: str, images: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¢å¼ºçš„å†…å®¹æ€»ç»“åŠŸèƒ½ï¼ŒåŒ…æ‹¬å†…å®¹æ€»ç»“ã€æƒ…æ„Ÿåˆ†æå’Œå…³é”®ä¿¡æ¯æå–
        
        Args:
            content: å¸–å­å†…å®¹
            title: å¸–å­æ ‡é¢˜
            images: å›¾ç‰‡åˆ—è¡¨
            
        Returns:
            åŒ…å«æ€»ç»“ã€æƒ…æ„Ÿåˆ†æå’Œå…³é”®ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹å¢å¼ºå†…å®¹æ€»ç»“: '{title[:30]}...'")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        try:
            # 1. å‡†å¤‡å®Œæ•´å†…å®¹ï¼ˆåŒ…æ‹¬å›¾ç‰‡OCRç»“æœï¼‰
            full_content = content
            if images and len(images) > 0:
                print(f"ğŸ“¸ åŒ…å« {len(images)} å¼ å›¾ç‰‡")
                # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„OCRåŠŸèƒ½è·å–å›¾ç‰‡å†…å®¹
                # ä¸ºäº†ç®€æ´ï¼Œæˆ‘ä»¬å‡è®¾å›¾ç‰‡å†…å®¹å·²ç»é€šè¿‡å…¶ä»–æ–¹å¼å¤„ç†
            
            # 2. è°ƒç”¨LLMè¿›è¡Œå¢å¼ºæ€»ç»“
            question = f'''è¯·å¯¹è¿™ç¯‡å†…å®¹è¿›è¡Œå¢å¼ºæ€»ç»“ï¼Œè¾“å‡ºæ ¼å¼ä¸ºJSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - summary: ä¸»è¦å†…å®¹æ€»ç»“ï¼ˆ200å­—ä»¥å†…ï¼‰
            - sentiment: æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/ä¸­æ€§/æ¶ˆæï¼‰
            - key_points: å…³é”®ä¿¡æ¯åˆ—è¡¨ï¼ˆ5-10ä¸ªè¦ç‚¹ï¼‰
            - category: å†…å®¹ç±»åˆ«
            - difficulty: éš¾åº¦çº§åˆ«ï¼ˆåˆçº§/ä¸­çº§/é«˜çº§ï¼‰
            
            å†…å®¹ï¼š
            æ ‡é¢˜ï¼š{title}
            æ­£æ–‡ï¼š{full_content}
            '''
            
            # ä½¿ç”¨gemini_ocr.pyå·¥å…·è¿›è¡Œæ€»ç»“
            result = self._call_llm_tool(question)
            
            if result:
                try:
                    # è§£æJSONç»“æœ
                    summary_data = json.loads(result)
                    return summary_data
                except json.JSONDecodeError:
                    # å¦‚æœLLMè¿”å›çš„ä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
                    return self._extract_summary_info(result, title)
            
            return {}
            
        except Exception as e:
            print(f"âŒ å¢å¼ºæ€»ç»“å¼‚å¸¸: {type(e).__name__}: {e}")
            return {}
    
    def _call_llm_tool(self, question: str) -> str:
        """
        è°ƒç”¨LLMå·¥å…·
        
        Args:
            question: é—®é¢˜å†…å®¹
            
        Returns:
            LLMå›ç­”
        """
        try:
            args = [
                OCR_CONFIG["python_path"],
                OCR_TOOL,
                "--question",
                question
            ]
            
            result = subprocess.run(
                args,
                shell=False,
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=120
            )
            
            if result.returncode != 0:
                print(f"âŒ LLMå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"ğŸ’¬ é”™è¯¯è¾“å‡º: {result.stderr}")
                return ""
            
            # æå–ç»“æœ
            output = result.stdout.strip()
            if "=== å¤„ç†ç»“æœ ===" in output:
                result_part = output.split("=== å¤„ç†ç»“æœ ===")[1]
                if "å›ç­”: " in result_part:
                    return result_part.split("å›ç­”: ")[1].strip()
            
            return output
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLMå·¥å…·å¼‚å¸¸: {type(e).__name__}: {e}")
            return ""
    
    def _extract_summary_info(self, text: str, title: str) -> Dict[str, Any]:
        """
        ä»éJSONæ ¼å¼çš„æ–‡æœ¬ä¸­æå–æ€»ç»“ä¿¡æ¯
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            title: å¸–å­æ ‡é¢˜
            
        Returns:
            æå–çš„æ€»ç»“ä¿¡æ¯
        """
        # ç®€å•çš„ä¿¡æ¯æå–é€»è¾‘
        return {
            "summary": text[:200] + "..." if len(text) > 200 else text,
            "sentiment": "ä¸­æ€§",
            "key_points": [text[:100] + "..."],
            "category": "æœªåˆ†ç±»",
            "difficulty": "ä¸­çº§"
        }
    
    def analyze_image_content(self, image_url: str) -> Dict[str, Any]:
        """
        åˆ†æå›¾åƒå†…å®¹ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œæ ‡ç­¾æå–
        
        Args:
            image_url: å›¾åƒURL
            
        Returns:
            åŒ…å«å›¾åƒåˆ†æç»“æœçš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹å›¾åƒå†…å®¹åˆ†æ: {image_url[:50]}...")
        
        try:
            # ä¸‹è½½å›¾åƒ
            temp_dir = "/tmp/xhs_image_analysis"
            os.makedirs(temp_dir, exist_ok=True)
            img_save_path = os.path.join(temp_dir, f"image_{int(time.time())}.jpg")
            
            import requests
            response = requests.get(image_url, timeout=30)
            response.raise_for_status()
            with open(img_save_path, 'wb') as f:
                f.write(response.content)
            
            # è°ƒç”¨å›¾åƒåˆ†æå·¥å…·
            question = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š1. å›¾åƒä¸»è¦å†…å®¹ï¼›2. ç›¸å…³æ ‡ç­¾ï¼ˆ5-10ä¸ªï¼‰ï¼›3. å›¾åƒç±»åˆ«ï¼›4. å…³é”®å…ƒç´ æè¿°"
            
            args = [
                OCR_CONFIG["python_path"],
                OCR_TOOL,
                img_save_path,
                "--question",
                question
            ]
            
            result = subprocess.run(
                args,
                shell=False,
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=120
            )
            
            if result.returncode != 0:
                print(f"âŒ å›¾åƒåˆ†æå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                print(f"ğŸ’¬ é”™è¯¯è¾“å‡º: {result.stderr}")
                return {}
            
            # æå–åˆ†æç»“æœ
            output = result.stdout.strip()
            if "=== å¤„ç†ç»“æœ ===" in output:
                result_part = output.split("=== å¤„ç†ç»“æœ ===")[1]
                if "å›ç­”: " in result_part:
                    analysis_text = result_part.split("å›ç­”: ")[1].strip()
                    
                    # è§£æç»“æœï¼ˆç®€å•ç¤ºä¾‹ï¼‰
                    return {
                        "content": analysis_text,
                        "tags": [analysis_text[:20] for _ in range(5)],  # ç®€åŒ–å¤„ç†
                        "category": "æœªåˆ†ç±»",
                        "elements": [analysis_text[:50]]
                    }
            
            return {
                "content": output,
                "tags": [],
                "category": "æœªåˆ†ç±»",
                "elements": []
            }
            
        except Exception as e:
            print(f"âŒ å›¾åƒå†…å®¹åˆ†æå¼‚å¸¸: {type(e).__name__}: {e}")
            return {}
    
    def build_content_index(self, posts: List[Dict[str, Any]]):
        """
        æ„å»ºå†…å®¹ç´¢å¼•ï¼Œç”¨äºç›¸ä¼¼åº¦æœç´¢
        
        Args:
            posts: å¸–å­åˆ—è¡¨
        """
        print(f"ğŸ”§ å¼€å§‹æ„å»ºå†…å®¹ç´¢å¼•ï¼Œå…± {len(posts)} ç¯‡å¸–å­")
        
        self.posts = posts
        
        # æå–å¸–å­å†…å®¹
        post_contents = []
        for post in posts:
            content = ""
            # ä» basic_info è·å–æ ‡é¢˜
            if "basic_info" in post:
                title = post["basic_info"].get("title", "")
                content += title + " "
            # ä» detail è·å–æ­£æ–‡
            if "detail" in post:
                desc = post["detail"].get("desc", "")
                content += desc
            post_contents.append(content)
        
        # æ„å»ºTF-IDFå‘é‡
        self.post_vectors = self.tfidf_vectorizer.fit_transform(post_contents)
        print(f"âœ… å†…å®¹ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def search_similar_posts(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹ç›¸ä¼¼åº¦æœç´¢å¸–å­
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼åº¦æ’åºçš„å¸–å­åˆ—è¡¨
        """
        if self.post_vectors is None or self.posts is None:
            print("âŒ å†…å®¹ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_content_index")
            return []
        
        print(f"ğŸ” å¼€å§‹ç›¸ä¼¼åº¦æœç´¢: '{query}'")
        
        # è½¬æ¢æŸ¥è¯¢å‘é‡
        query_vector = self.tfidf_vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.post_vectors).flatten()
        
        # è·å–top-kç»“æœ
        top_indices = similarities.argsort()[::-1][:top_k]
        
        # æ„å»ºç»“æœ
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # åªè¿”å›ç›¸ä¼¼åº¦å¤§äº0çš„ç»“æœ
                post = self.posts[idx].copy()
                post["similarity"] = float(similarities[idx])
                results.append(post)
        
        print(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³å¸–å­")
        return results
    
    def recommend_posts(self, post_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹ç›¸ä¼¼åº¦æ¨èå¸–å­
        
        Args:
            post_id: å‚è€ƒå¸–å­ID
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ¨èå¸–å­åˆ—è¡¨
        """
        if self.post_vectors is None or self.posts is None:
            print("âŒ å†…å®¹ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_content_index")
            return []
        
        print(f"ğŸ” å¼€å§‹æ¨èå¸–å­ï¼Œå‚è€ƒID: {post_id}")
        
        # æ‰¾åˆ°å‚è€ƒå¸–å­
        ref_idx = -1
        for i, post in enumerate(self.posts):
            if post.get("note_id") == post_id:
                ref_idx = i
                break
        
        if ref_idx == -1:
            print(f"âŒ æœªæ‰¾åˆ°å‚è€ƒå¸–å­: {post_id}")
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        ref_vector = self.post_vectors[ref_idx]
        similarities = cosine_similarity(ref_vector, self.post_vectors).flatten()
        
        # è·å–top-kç»“æœï¼ˆæ’é™¤è‡ªèº«ï¼‰
        top_indices = similarities.argsort()[::-1][1:top_k+1]
        
        # æ„å»ºç»“æœ
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # åªè¿”å›ç›¸ä¼¼åº¦å¤§äº0çš„ç»“æœ
                post = self.posts[idx].copy()
                post["similarity"] = float(similarities[idx])
                results.append(post)
        
        print(f"âœ… ç”Ÿæˆ {len(results)} ç¯‡æ¨èå¸–å­")
        return results
    
    def analyze_trends(self, posts: List[Dict[str, Any]], time_window: str = "month") -> Dict[str, Any]:
        """
        åˆ†æå†…å®¹è¶‹åŠ¿
        
        Args:
            posts: å¸–å­åˆ—è¡¨
            time_window: æ—¶é—´çª—å£ï¼ˆday/week/monthï¼‰
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        print(f"ğŸ“Š å¼€å§‹è¶‹åŠ¿åˆ†æï¼Œå…± {len(posts)} ç¯‡å¸–å­ï¼Œæ—¶é—´çª—å£: {time_window}")
        
        # ç®€åŒ–çš„è¶‹åŠ¿åˆ†æ
        # 1. è®¡ç®—å„ç±»åˆ«çš„å¸–å­æ•°é‡
        category_counts = {}
        for post in posts:
            category = post.get("category", "æœªåˆ†ç±»")
            category_counts[category] = category_counts.get(category, 0) + 1
        
        # 2. è®¡ç®—å…³é”®è¯é¢‘ç‡
        all_content = " "
        for post in posts:
            if "title" in post:
                all_content += post["title"] + " "
            if "content" in post:
                all_content += post["content"] + " "
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
        words = re.findall(r'\b\w{2,}\b', all_content)
        word_counts = {}
        for word in words:
            # æ’é™¤å¸¸è§åœç”¨è¯
            if word not in ["çš„", "äº†", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"]:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # æ’åºå…³é”®è¯
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return {
            "category_distribution": category_counts,
            "top_keywords": sorted_words,
            "total_posts": len(posts)
        }


def get_ai_utils() -> AIUtils:
    """
    è·å–AIå·¥å…·å®ä¾‹
    
    Returns:
        AIUtilså®ä¾‹
    """
    return AIUtils()
