#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹é¢è¯•é¢˜åº“æ¨¡å—
æä¾›é¢˜åº“ç®¡ç†ã€AIåˆ†ç±»å’Œåˆ·é¢˜åŠŸèƒ½
"""

import os
import sys
import json
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
from xhs_crawler.core.base_crawler import BaseCrawler
from xhs_crawler.core.ai_utils import AIUtils


class Difficulty(Enum):
    """éš¾åº¦çº§åˆ«"""
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class QuestionType(Enum):
    """é¢˜ç›®ç±»å‹"""
    SINGLE_CHOICE = "single_choice"
    MULTIPLE_CHOICE = "multiple_choice"
    FILL_BLANK = "fill_blank"
    TRUE_FALSE = "true_false"
    DISCUSSION = "discussion"


@dataclass
class Question:
    """é¢è¯•é¢˜æ•°æ®ç»“æ„"""
    id: str
    content: str
    answer: str
    category: str
    difficulty: str
    question_type: str
    options: List[str] = None
    explanation: str = ""
    source: str = ""
    created_at: str = ""
    
    def __post_init__(self):
        if self.options is None:
            self.options = []


class QuestionBank:
    """
    é¢è¯•é¢˜åº“ç®¡ç†ç±»
    
    æä¾›é¢˜åº“çš„æ„å»ºã€å­˜å‚¨ã€åˆ†ç±»å’Œåˆ·é¢˜åŠŸèƒ½
    """
    
    def __init__(self, output_dir: str = "question_bank"):
        """
        åˆå§‹åŒ–é¢˜åº“
        
        Args:
            output_dir: é¢˜åº“å­˜æ”¾ç›®å½•
        """
        self.output_dir = output_dir
        self.questions_file = os.path.join(output_dir, "questions.json")
        self.categories_file = os.path.join(output_dir, "categories.json")
        self.crawler = BaseCrawler(output_dir=output_dir)
        self.ai_utils = AIUtils()
        self.questions: List[Question] = []
        self.categories: Dict[str, Dict[str, Any]] = {}
        self._ensure_output_dirs()
    
    def _ensure_output_dirs(self) -> None:
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        os.makedirs(self.output_dir, exist_ok=True)
    
    def crawl_questions(self, keywords: List[str] = None, pages_per_keyword: int = 3) -> List[Question]:
        """
        ä»å°çº¢ä¹¦æŠ“å–é¢è¯•é¢˜
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            pages_per_keyword: æ¯ä¸ªå…³é”®è¯æŠ“å–çš„é¡µæ•°
            
        Returns:
            æŠ“å–çš„é¢˜ç›®åˆ—è¡¨
        """
        if keywords is None:
            keywords = [
                "å¤§æ¨¡å‹é¢è¯•é¢˜",
                "LLMé¢è¯•",
                "transformeré¢è¯•é¢˜",
                "æ·±åº¦å­¦ä¹ é¢è¯•",
                "AIç®—æ³•é¢è¯•"
            ]
        
        print("=" * 60)
        print("ğŸ“š å¼€å§‹æŠ“å–é¢è¯•é¢˜åº“")
        print("=" * 60)
        
        all_questions = []
        
        for keyword in keywords:
            print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            for page in range(1, pages_per_keyword + 1):
                print(f"   ğŸ“„ ç¬¬ {page}/{pages_per_keyword} é¡µ...")
                
                notes = self.crawler.search_posts(keywords=keyword, page_num=page, page_size=10)
                
                for note in notes:
                    note_id = note.get("note_id")
                    if not note_id:
                        continue
                    
                    detail = self.crawler.get_post_detail(
                        note_id=note_id,
                        xsec_token=note.get("xsec_token", ""),
                        xsec_source=note.get("xsec_source", "pc_feed")
                    )
                    
                    if not detail:
                        continue
                    
                    content = detail.get("desc", "")
                    title = note.get("title", "")
                    
                    if len(content) < 50:
                        continue
                    
                    question = self._extract_question(note, detail, content, title)
                    if question:
                        all_questions.append(question)
                
                if notes:
                    print(f"   âœ… å¤„ç† {len(notes)} ç¯‡å¸–å­")
        
        print(f"\nğŸ‰ å…±æŠ“å– {len(all_questions)} é“é¢è¯•é¢˜")
        self.questions = all_questions
        return all_questions
    
    def _extract_question(self, note: Dict, detail: Dict, content: str, title: str) -> Optional[Question]:
        """
        ä»å¸–å­ä¸­æå–é¢è¯•é¢˜
        
        Args:
            note: å¸–å­åŸºæœ¬ä¿¡æ¯
            detail: å¸–å­è¯¦æƒ…
            content: æ­£æ–‡å†…å®¹
            title: æ ‡é¢˜
            
        Returns:
            é¢è¯•é¢˜å¯¹è±¡
        """
        import hashlib
        from datetime import datetime
        
        note_id = note.get("note_id", "")
        question_id = hashlib.md5(f"{note_id}_{title}".encode()).hexdigest()[:12]
        
        content_clean = content.strip()
        
        lines = content_clean.split('\n')
        question_lines = []
        answer_lines = []
        is_answer = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if any(keyword in line.lower() for keyword in ['ç­”æ¡ˆ', 'è§£ç­”', 'è§£æ', 'ç­”:', 'ç­”æ¡ˆ:']):
                is_answer = True
                continue
            
            if is_answer:
                answer_lines.append(line)
            else:
                question_lines.append(line)
        
        if not answer_lines:
            answer_lines = ["å‚è€ƒç­”æ¡ˆè§åŸæ–‡"]
        
        question_content = '\n'.join(question_lines)
        answer_content = '\n'.join(answer_lines)
        
        return Question(
            id=question_id,
            content=question_content,
            answer=answer_content,
            category="å¾…åˆ†ç±»",
            difficulty="medium",
            question_type="discussion",
            source=title,
            created_at=datetime.now().isoformat()
        )
    
    def categorize_questions(self) -> Dict[str, Dict[str, Any]]:
        """
        ä½¿ç”¨ AI å¯¹é¢˜ç›®è¿›è¡Œåˆ†ç±»
        
        Returns:
            åˆ†ç±»ç»“æœ
        """
        print("\n" + "=" * 60)
        print("ğŸ§  ä½¿ç”¨ AI å¯¹é¢è¯•é¢˜è¿›è¡Œåˆ†ç±»")
        print("=" * 60)
        
        if not self.questions:
            print("âŒ æ²¡æœ‰é¢˜ç›®éœ€è¦åˆ†ç±»")
            return {}
        
        category_mapping = {
            "transformer": ["Transformeræ¶æ„", "Attentionæœºåˆ¶", "ä½ç½®ç¼–ç ", "Encoder-Decoder"],
            "llm_fundamentals": ["LLMåŸºç¡€", "å¤§è¯­è¨€æ¨¡å‹", "é¢„è®­ç»ƒ", "å¾®è°ƒ"],
            "rlhf": ["RLHF", "å¼ºåŒ–å­¦ä¹ ", "å¯¹é½", "PPO"],
            "rag": ["RAG", "æ£€ç´¢å¢å¼º", "å‘é‡æ•°æ®åº“", "çŸ¥è¯†åº“"],
            "prompt_engineering": ["æç¤ºå·¥ç¨‹", "Prompt", "few-shot"],
            "deployment": ["éƒ¨ç½²", "æ¨ç†", "é‡åŒ–", "åŠ é€Ÿ"],
            "coding": ["ä»£ç ", "å®ç°", "Python", "PyTorch"],
            "math": ["æ•°å­¦", "æ¦‚ç‡è®º", "çº¿æ€§ä»£æ•°", "ä¼˜åŒ–"],
            "nlp": ["NLP", "è‡ªç„¶è¯­è¨€å¤„ç†", "åˆ†è¯", "è¯å‘é‡"]
        }
        
        categorized = {}
        
        for i, question in enumerate(self.questions):
            print(f"   ğŸ“ å¤„ç†é¢˜ç›® {i+1}/{len(self.questions)}...")
            
            content = question.content[:500]
            title = question.source
            
            category = self._classify_single_question(content, title, category_mapping)
            
            question.category = category
            
            if category not in categorized:
                categorized[category] = {
                    "name": self._get_category_name(category),
                    "count": 0,
                    "questions": []
                }
            
            categorized[category]["count"] += 1
            categorized[category]["questions"].append(asdict(question))
        
        self.categories = categorized
        print(f"\nâœ… åˆ†ç±»å®Œæˆï¼Œå…± {len(categorized)} ä¸ªç±»åˆ«")
        
        for cat, info in categorized.items():
            print(f"   - {info['name']}: {info['count']} é¢˜")
        
        return categorized
    
    def _classify_single_question(self, content: str, title: str, mapping: Dict[str, List[str]]) -> str:
        """
        åˆ†ç±»å•ä¸ªé¢˜ç›®
        
        Args:
            content: é¢˜ç›®å†…å®¹
            title: é¢˜ç›®æ¥æºæ ‡é¢˜
            mapping: åˆ†ç±»æ˜ å°„
            
        Returns:
            åˆ†ç±»æ ‡ç­¾
        """
        text = (title + " " + content).lower()
        
        scores = {}
        for cat, keywords in mapping.items():
            score = sum(1 for kw in keywords if kw.lower() in text)
            scores[cat] = score
        
        max_score_cat = max(scores, key=scores.get) if scores else "other"
        
        if scores.get(max_score_cat, 0) == 0:
            return "other"
        
        return max_score_cat
    
    def _get_category_name(self, category: str) -> str:
        """
        è·å–åˆ†ç±»æ˜¾ç¤ºåç§°
        
        Args:
            category: åˆ†ç±»æ ‡ç­¾
            
        Returns:
            æ˜¾ç¤ºåç§°
        """
        names = {
            "transformer": "Transformeræ¶æ„",
            "llm_fundamentals": "LLMåŸºç¡€ç†è®º",
            "rlhf": "RLHFä¸å¯¹é½",
            "rag": "RAGæ£€ç´¢å¢å¼º",
            "prompt_engineering": "æç¤ºå·¥ç¨‹",
            "deployment": "æ¨¡å‹éƒ¨ç½²",
            "coding": "ç¼–ç¨‹å®ç°",
            "math": "æ•°å­¦åŸºç¡€",
            "nlp": "NLPçŸ¥è¯†",
            "other": "å…¶ä»–é¢˜ç›®"
        }
        return names.get(category, category)
    
    def _get_default_categories(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–é»˜è®¤åˆ†ç±»å®šä¹‰
        
        Returns:
            é»˜è®¤åˆ†ç±»å­—å…¸ï¼ŒåŒ…å«åˆ†ç±»æ ‡ç­¾ã€åç§°ã€é¢˜ç›®æ•°é‡å’Œé¢˜ç›®åˆ—è¡¨
        """
        return {
            "transformer": {
                "name": "Transformeræ¶æ„",
                "count": 0,
                "questions": []
            },
            "llm_fundamentals": {
                "name": "LLMåŸºç¡€ç†è®º",
                "count": 0,
                "questions": []
            },
            "rlhf": {
                "name": "RLHFä¸å¯¹é½",
                "count": 0,
                "questions": []
            },
            "rag": {
                "name": "RAGæ£€ç´¢å¢å¼º",
                "count": 0,
                "questions": []
            },
            "prompt_engineering": {
                "name": "æç¤ºå·¥ç¨‹",
                "count": 0,
                "questions": []
            },
            "deployment": {
                "name": "æ¨¡å‹éƒ¨ç½²",
                "count": 0,
                "questions": []
            },
            "coding": {
                "name": "ç¼–ç¨‹å®ç°",
                "count": 0,
                "questions": []
            },
            "math": {
                "name": "æ•°å­¦åŸºç¡€",
                "count": 0,
                "questions": []
            },
            "nlp": {
                "name": "NLPçŸ¥è¯†",
                "count": 0,
                "questions": []
            },
            "other": {
                "name": "å…¶ä»–é¢˜ç›®",
                "count": 0,
                "questions": []
            }
        }
    
    def save(self) -> None:
        """ä¿å­˜é¢˜åº“åˆ°æ–‡ä»¶"""
        print(f"\nğŸ’¾ ä¿å­˜é¢˜åº“åˆ° {self.output_dir}")
        
        questions_data = [asdict(q) for q in self.questions]
        with open(self.questions_file, 'w', encoding='utf-8') as f:
            json.dump(questions_data, f, ensure_ascii=False, indent=2)
        
        with open(self.categories_file, 'w', encoding='utf-8') as f:
            json.dump(self.categories, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¿å­˜å®Œæˆ")
        print(f"   - é¢˜ç›®æ€»æ•°: {len(self.questions)}")
        print(f"   - åˆ†ç±»æ•°: {len(self.categories)}")
    
    def load(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½é¢˜åº“
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not os.path.exists(self.questions_file):
            print(f"âŒ é¢˜åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.questions_file}")
            return False
        
        with open(self.questions_file, 'r', encoding='utf-8') as f:
            questions_data = json.load(f)
        
        self.questions = [Question(**q) for q in questions_data]
        
        if os.path.exists(self.categories_file):
            with open(self.categories_file, 'r', encoding='utf-8') as f:
                self.categories = json.load(f)
        
        print(f"âœ… åŠ è½½é¢˜åº“æˆåŠŸ")
        print(f"   - é¢˜ç›®æ€»æ•°: {len(self.questions)}")
        print(f"   - åˆ†ç±»æ•°: {len(self.categories)}")
        
        return True
    
    def get_practice_questions(self, category: str = None, difficulty: str = None, count: int = 10) -> List[Question]:
        """
        è·å–åˆ·é¢˜é¢˜ç›®
        
        Args:
            category: åˆ†ç±»ç­›é€‰
            difficulty: éš¾åº¦ç­›é€‰
            count: é¢˜ç›®æ•°é‡
            
        Returns:
            éšæœºç­›é€‰çš„é¢˜ç›®åˆ—è¡¨
        """
        filtered = self.questions
        
        if category and category != "all":
            filtered = [q for q in filtered if q.category == category]
        
        if difficulty:
            filtered = [q for q in filtered if q.difficulty == difficulty]
        
        if len(filtered) <= count:
            return filtered
        
        return random.sample(filtered, count)
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–é¢˜åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "total": len(self.questions),
            "by_category": {},
            "by_difficulty": {},
            "by_type": {}
        }
        
        for q in self.questions:
            stats["by_category"][q.category] = stats["by_category"].get(q.category, 0) + 1
            stats["by_difficulty"][q.difficulty] = stats["by_difficulty"].get(q.difficulty, 0) + 1
            stats["by_type"][q.question_type] = stats["by_type"].get(q.question_type, 0) + 1
        
        return stats
