#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦å¤šå…³é”®è¯å¹¶è¡Œçˆ¬è™«
ä½¿ç”¨ ThreadPoolExecutor å®ç°å…³é”®è¯å¹¶è¡Œçˆ¬å–ï¼Œå¤§å¹…æå‡çˆ¬å–æ•ˆç‡
"""

import time
import logging
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from concurrent.futures._base import TimeoutError
from dataclasses import dataclass
from datetime import datetime
import threading

from xhs_crawler.core.base_crawler import BaseCrawler
from xhs_crawler.core.config import get_output_dir, get_html_file_path, DEFAULT_CRAWLER_CONFIG

logger = logging.getLogger(__name__)


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœæ•°æ®ç±»"""
    keyword: str
    success: bool
    notes: List[Dict[str, Any]]
    error: Optional[str] = None
    duration: float = 0.0
    pages_crawled: int = 0


class ParallelKeywordCrawler(BaseCrawler):
    """
    å¤šå…³é”®è¯å¹¶è¡Œçˆ¬è™«
    
    ä½¿ç”¨ ThreadPoolExecutor å®ç°å…³é”®è¯çº§åˆ«çš„å¹¶è¡Œçˆ¬å–ï¼Œ
    åŒæ—¶æ”¯æŒæ‰¹é‡è·å–å¸–å­è¯¦æƒ…ï¼Œæ˜¾è‘—æå‡çˆ¬å–æ•ˆç‡ã€‚
    
    Features:
        - å…³é”®è¯çº§åˆ«å¹¶è¡Œçˆ¬å–
        - å¸–å­è¯¦æƒ…æ‰¹é‡å¹¶å‘è·å–
        - æ™ºèƒ½è¶…æ—¶æ§åˆ¶
        - è¿›åº¦å®æ—¶è·Ÿè¸ª
        - é”™è¯¯è‡ªåŠ¨æ¢å¤
    """
    
    def __init__(
        self,
        max_workers: int = 5,
        detail_concurrency: int = 10,
        timeout_per_keyword: float = 120.0,
        timeout_per_detail: float = 30.0
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œçˆ¬è™«
        
        Args:
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆå…³é”®è¯çˆ¬å–ï¼‰
            detail_concurrency: è¯¦æƒ…è·å–å¹¶å‘æ•°
            timeout_per_keyword: æ¯ä¸ªå…³é”®è¯çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            timeout_per_detail: æ¯ä¸ªè¯¦æƒ…è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        output_dir = get_output_dir("parallel_multi_keyword")
        super().__init__(output_dir)
        self.html_file = get_html_file_path("parallel_multi_keyword")
        self.config = DEFAULT_CRAWLER_CONFIG
        
        self.max_workers = max_workers
        self.detail_concurrency = detail_concurrency
        self.timeout_per_keyword = timeout_per_keyword
        self.timeout_per_detail = timeout_per_detail
        
        self._total_notes_lock = threading.Lock()
        self._progress_lock = threading.Lock()
        self._total_notes: List[Dict[str, Any]] = []
        self._crawl_stats = {
            "keywords_processed": 0,
            "keywords_failed": 0,
            "total_pages": 0,
            "total_notes": 0,
            "start_time": None,
            "end_time": None
        }
    
    def _crawl_single_keyword(
        self,
        keyword: str,
        max_pages: int,
        page_size: int
    ) -> CrawlResult:
        """
        çˆ¬å–å•ä¸ªå…³é”®è¯çš„æ‰€æœ‰é¡µé¢
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            CrawlResult: çˆ¬å–ç»“æœ
        """
        start_time = time.time()
        notes = []
        pages_crawled = 0
        
        try:
            for page_num in range(1, max_pages + 1):
                page_start = time.time()
                
                result = self.search_posts(keyword, page_num=page_num, page_size=page_size)
                page_notes = result if isinstance(result, list) else []
                
                if not page_notes:
                    logger.info(f"å…³é”®è¯ '{keyword}' ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                notes.extend(page_notes)
                pages_crawled += 1
                
                page_duration = time.time() - page_start
                logger.info(
                    f"å…³é”®è¯ '{keyword}' ç¬¬ {page_num}/{max_pages} é¡µ: "
                    f"è·å– {len(page_notes)} ç¯‡ç¬”è®° (è€—æ—¶: {page_duration:.2f}s)"
                )
                
                time.sleep(self.config["sleep_time"])
            
            duration = time.time() - start_time
            return CrawlResult(
                keyword=keyword,
                success=True,
                notes=notes,
                duration=duration,
                pages_crawled=pages_crawled
            )
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"å…³é”®è¯ '{keyword}' çˆ¬å–å¤±è´¥: {e}")
            return CrawlResult(
                keyword=keyword,
                success=False,
                notes=[],
                error=str(e),
                duration=duration,
                pages_crawled=pages_crawled
            )
    
    def _get_single_detail(
        self,
        note: Dict[str, Any],
        index: int,
        total: int
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–å•ä¸ªå¸–å­è¯¦æƒ…
        
        Args:
            note: å¸–å­åŸºæœ¬ä¿¡æ¯
            index: å½“å‰ç´¢å¼•
            total: æ€»æ•°
            
        Returns:
            Optional[Dict]: å¸–å­è¯¦æƒ…
        """
        note_id = note.get("note_id")
        if not note_id:
            return None
        
        try:
            detail = self.get_post_detail(
                note_id=note_id,
                xsec_token=note.get("xsec_token", ""),
                xsec_source=note.get("xsec_source", "pc_feed")
            )
            
            if detail:
                logger.debug(f"è¯¦æƒ…è·å–æˆåŠŸ: {note_id} ({index}/{total})")
                return {
                    "basic_info": note,
                    "detail": detail
                }
            return None
            
        except Exception as e:
            logger.warning(f"è¯¦æƒ…è·å–å¤±è´¥: {note_id}, é”™è¯¯: {e}")
            return None
    
    def _batch_fetch_details(
        self,
        notes: List[Dict[str, Any]],
        concurrency: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å¸–å­è¯¦æƒ…
        
        Args:
            notes: å¸–å­åˆ—è¡¨
            concurrency: å¹¶å‘æ•°ï¼Œé»˜è®¤ä½¿ç”¨å®ä¾‹é…ç½®
            
        Returns:
            List[Dict]: åŒ…å«è¯¦æƒ…çš„å¸–å­åˆ—è¡¨
        """
        if not notes:
            return []
        
        max_workers = concurrency or self.detail_concurrency
        posts = []
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(notes)} ç¯‡å¸–å­è¯¦æƒ…ï¼Œå¹¶å‘æ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_note = {
                executor.submit(
                    self._get_single_detail,
                    note,
                    i + 1,
                    len(notes)
                ): note for i, note in enumerate(notes)
            }
            
            completed = 0
            total = len(future_to_note)
            
            for future in as_completed(future_to_note):
                completed += 1
                
                try:
                    result = future.result(timeout=self.timeout_per_detail)
                    if result:
                        posts.append(result)
                except TimeoutError:
                    logger.warning(f"è¯¦æƒ…è·å–è¶…æ—¶ (ç´¢å¼• {completed}/{total})")
                except Exception as e:
                    logger.error(f"è¯¦æƒ…å¤„ç†å¼‚å¸¸: {e}")
                
                if completed % 10 == 0 or completed == total:
                    progress = (completed / total) * 100
                    logger.info(f"è¯¦æƒ…è·å–è¿›åº¦: {completed}/{total} ({progress:.1f}%)")
        
        logger.info(f"è¯¦æƒ…è·å–å®Œæˆ: æˆåŠŸ {len(posts)}/{len(notes)} ç¯‡")
        return posts
    
    def run(
        self,
        keywords: Optional[List[str]] = None,
        max_pages: Optional[int] = None,
        page_size: Optional[int] = None,
        max_workers: Optional[int] = None,
        detail_concurrency: Optional[int] = None,
        enable_detail_fetch: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå¹¶è¡Œçˆ¬è™«
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            max_pages: æ¯ä¸ªå…³é”®è¯çˆ¬å–çš„æœ€å¤§é¡µæ•°
            page_size: æ¯é¡µç»“æœæ•°é‡
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            detail_concurrency: è¯¦æƒ…è·å–å¹¶å‘æ•°
            enable_detail_fetch: æ˜¯å¦è·å–å¸–å­è¯¦æƒ…
            
        Returns:
            Dict: çˆ¬å–ç»“æœç»Ÿè®¡
        """
        self._crawl_stats["start_time"] = datetime.now()
        
        if keywords is None:
            keywords = ["å¤§æ¨¡å‹", "é¢è¯•", "ç»éªŒåˆ†äº«"]
        if max_pages is None:
            max_pages = self.config["max_pages"]
        if page_size is None:
            page_size = self.config["page_size"]
        if max_workers is None:
            max_workers = self.max_workers
        if detail_concurrency is None:
            detail_concurrency = self.detail_concurrency
        
        max_workers = min(max_workers, len(keywords))
        
        logger.info("=" * 60)
        logger.info("ğŸš€ å¯åŠ¨å°çº¢ä¹¦å¤šå…³é”®è¯å¹¶è¡Œçˆ¬è™«")
        logger.info(f"ğŸ“ å…³é”®è¯æ•°é‡: {len(keywords)}")
        logger.info(f"âš¡ æœ€å¤§å¹¶è¡Œæ•°: {max_workers}")
        logger.info(f"ğŸ“„ æ¯é¡µæ•°é‡: {page_size}")
        logger.info(f"ğŸ“‘ æœ€å¤§é¡µæ•°/å…³é”®è¯: {max_pages}")
        if enable_detail_fetch:
            logger.info(f"ğŸ”— è¯¦æƒ…å¹¶å‘æ•°: {detail_concurrency}")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        all_results: List[CrawlResult] = []
        
        logger.info("\nğŸ“Š ç¬¬ä¸€é˜¶æ®µ: å¹¶è¡Œçˆ¬å–æœç´¢ç»“æœ")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_keyword = {}
            
            for keyword in keywords:
                future = executor.submit(
                    self._crawl_single_keyword,
                    keyword,
                    max_pages,
                    page_size
                )
                future_to_keyword[future] = keyword
            
            for future in as_completed(future_to_keyword):
                keyword = future_to_keyword[future]
                
                try:
                    result = future.result(timeout=self.timeout_per_keyword)
                    all_results.append(result)
                    
                    with self._progress_lock:
                        self._crawl_stats["keywords_processed"] += 1
                        self._crawl_stats["total_pages"] += result.pages_crawled
                        self._crawl_stats["total_notes"] += len(result.notes)
                    
                    if result.success:
                        logger.info(
                            f"âœ… å…³é”®è¯ '{keyword}' å®Œæˆ: "
                            f"{result.pages_crawled} é¡µ, {len(result.notes)} ç¯‡ç¬”è®°, "
                            f"è€—æ—¶ {result.duration:.2f}s"
                        )
                    else:
                        with self._progress_lock:
                            self._crawl_stats["keywords_failed"] += 1
                        logger.warning(
                            f"âŒ å…³é”®è¯ '{keyword}' å¤±è´¥: {result.error}, "
                            f"è€—æ—¶ {result.duration:.2f}s"
                        )
                            
                except TimeoutError:
                    logger.error(f"â° å…³é”®è¯ '{keyword}' è¶…æ—¶")
                    all_results.append(CrawlResult(
                        keyword=keyword,
                        success=False,
                        notes=[],
                        error="Timeout",
                        pages_crawled=0
                    ))
                    with self._progress_lock:
                        self._crawl_stats["keywords_failed"] += 1
                        
                except Exception as e:
                    logger.error(f"ğŸ’¥ å…³é”®è¯ '{keyword}' å¼‚å¸¸: {e}")
                    all_results.append(CrawlResult(
                        keyword=keyword,
                        success=False,
                        notes=[],
                        error=str(e),
                        pages_crawled=0
                    ))
                    with self._progress_lock:
                        self._crawl_stats["keywords_failed"] += 1
        
        for result in all_results:
            self._total_notes.extend(result.notes)
        
        unique_notes = self.deduplicate_notes(self._total_notes)
        
        logger.info(
            f"\nğŸ“ˆ æœç´¢é˜¶æ®µå®Œæˆ: {len(self._total_notes)} ç¯‡ -> å»é‡å {len(unique_notes)} ç¯‡"
        )
        
        posts = []
        if enable_detail_fetch and unique_notes:
            logger.info(f"\nğŸ”— ç¬¬äºŒé˜¶æ®µ: å¹¶è¡Œè·å–å¸–å­è¯¦æƒ… (å¹¶å‘æ•°: {detail_concurrency})")
            detail_start = time.time()
            
            posts = self._batch_fetch_details(unique_notes, detail_concurrency)
            
            detail_duration = time.time() - detail_start
            logger.info(f"è¯¦æƒ…è·å–è€—æ—¶: {detail_duration:.2f}s")
            
            for i, post in enumerate(posts):
                title = post.get("basic_info", {}).get("title", f"å¸–å­{i+1}")
                clean_title = self._clean_filename(title)
                filename = f"{i+1:03d}_{clean_title}_detail.json"
                self._save_json_data(post, f"{self.detail_dir}/{filename}")
        
        self._crawl_stats["end_time"] = datetime.now()
        total_duration = time.time() - start_time
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ çˆ¬è™«å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_duration:.2f} ç§’")
        logger.info(f"ğŸ“ å¤„ç†å…³é”®è¯: {len(keywords)} ä¸ª")
        logger.info(f"âœ… æˆåŠŸå…³é”®è¯: {self._crawl_stats['keywords_processed'] - self._crawl_stats['keywords_failed']} ä¸ª")
        logger.info(f"âŒ å¤±è´¥å…³é”®è¯: {self._crawl_stats['keywords_failed']} ä¸ª")
        logger.info(f"ğŸ“„ æ€»é¡µæ•°: {self._crawl_stats['total_pages']}")
        logger.info(f"ğŸ“ æ€»ç¬”è®°æ•°: {self._crawl_stats['total_notes']} ç¯‡")
        logger.info(f"ğŸ”— è·å–è¯¦æƒ…: {len(posts)} ç¯‡")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {self.output_dir}")
        logger.info("=" * 60)
        
        if posts:
            self.generate_html_page(posts, self.html_file, "å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº« - å¹¶è¡Œçˆ¬å–")
            logger(f"ğŸŒ HTMLç½‘é¡µ: {self.html_file}")
        
        return {
            "status": "success",
            "total_duration": total_duration,
            "keywords": {
                "total": len(keywords),
                "successful": self._crawl_stats["keywords_processed"] - self._crawl_stats["keywords_failed"],
                "failed": self._crawl_stats["keywords_failed"]
            },
            "pages": self._crawl_stats["total_pages"],
            "notes": {
                "total": self._crawl_stats["total_notes"],
                "unique": len(unique_notes),
                "with_details": len(posts)
            },
            "output_dir": self.output_dir,
            "html_file": self.html_file if posts else None
        }
    
    def run_async(
        self,
        keywords: List[str],
        max_pages: int = 2,
        page_size: int = 10,
        max_workers: int = 5
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥è¿è¡Œå¹¶è¡Œçˆ¬è™«ï¼ˆéé˜»å¡ç‰ˆæœ¬ï¼‰
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            max_pages: æ¯ä¸ªå…³é”®è¯çˆ¬å–çš„æœ€å¤§é¡µæ•°
            page_size: æ¯é¡µç»“æœæ•°é‡
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            Dict: ä»»åŠ¡ä¿¡æ¯ï¼ˆä¸é˜»å¡ç­‰å¾…å®Œæˆï¼‰
        """
        import concurrent.futures
        
        executor = ThreadPoolExecutor(max_workers=max_workers)
        
        future = executor.submit(
            self.run,
            keywords=keywords,
            max_pages=max_pages,
            page_size=page_size,
            max_workers=max_workers
        )
        
        return {
            "executor": executor,
            "future": future,
            "status": "running"
        }
    
    def wait_for_result(self, task_info: Dict[str, Any], timeout: Optional[float] = None) -> Dict[str, Any]:
        """
        ç­‰å¾…å¼‚æ­¥ä»»åŠ¡å®Œæˆå¹¶è·å–ç»“æœ
        
        Args:
            task_info: ä»»åŠ¡ä¿¡æ¯å­—å…¸
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            Dict: çˆ¬å–ç»“æœ
        """
        future = task_info.get("future")
        executor = task_info.get("executor")
        
        if not future:
            return {"error": "Invalid task info"}
        
        try:
            result = future.result(timeout=timeout)
            if executor:
                executor.shutdown(wait=False)
            return result
        except TimeoutError:
            return {"error": "Task timeout"}
        except Exception as e:
            return {"error": str(e)}


def run_parallel_crawler(
    keywords: Optional[List[str]] = None,
    max_pages: int = 2,
    page_size: int = 10,
    max_workers: int = 5,
    detail_concurrency: int = 10
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡Œå¹¶è¡Œçˆ¬è™«
    
    Args:
        keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        max_pages: æ¯ä¸ªå…³é”®è¯çˆ¬å–çš„æœ€å¤§é¡µæ•°
        page_size: æ¯é¡µç»“æœæ•°é‡
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        detail_concurrency: è¯¦æƒ…è·å–å¹¶å‘æ•°
        
    Returns:
        Dict: çˆ¬å–ç»“æœç»Ÿè®¡
    """
    crawler = ParallelKeywordCrawler(
        max_workers=max_workers,
        detail_concurrency=detail_concurrency
    )
    return crawler.run(
        keywords=keywords,
        max_pages=max_pages,
        page_size=page_size,
        enable_detail_fetch=True
    )


if __name__ == "__main__":
    import json
    
    print("\n" + "=" * 60)
    print("ğŸš€ å°çº¢ä¹¦å¤šå…³é”®è¯å¹¶è¡Œçˆ¬è™«æ¼”ç¤º")
    print("=" * 60)
    
    result = run_parallel_crawler(
        keywords=["å¤§æ¨¡å‹é¢è¯•", "Transformeré¢è¯•", "æ·±åº¦å­¦ä¹ é¢è¯•"],
        max_pages=2,
        page_size=10,
        max_workers=3,
        detail_concurrency=5
    )
    
    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(json.dumps(result, ensure_ascii=False, indent=2))
