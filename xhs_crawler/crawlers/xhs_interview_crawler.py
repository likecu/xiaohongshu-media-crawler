#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«çˆ¬è™«è„šæœ¬
åŠŸèƒ½ï¼š
1. ä»é…ç½®æ–‡ä»¶è¯»å–å¤šä¸ªæœç´¢è¯
2. æœç´¢å°çº¢ä¹¦ä¸Šå…³äºå¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«çš„å¸–å­
3. è·å–æ¯ä¸ªå¸–å­çš„è¯¦ç»†å†…å®¹
4. å¯¹å¸–å­ä¸­çš„å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
5. ç”ŸæˆHTMLç½‘é¡µå±•ç¤ºæ‰€æœ‰å¸–å­
"""

import os
import json
import time
from typing import List, Dict, Any
from xhs_crawler.core.base_crawler import BaseCrawler
from xhs_crawler.core.config import get_output_dir, get_html_file_path, DEFAULT_SEARCH_CONFIG, OCR_CONFIG
from xhs_crawler.core.mcp_utils import load_json_data, save_json_data


class XhsInterviewCrawler(BaseCrawler):
    """
    å°çº¢ä¹¦å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«çˆ¬è™«ï¼Œç»§æ‰¿è‡ªBaseCrawler
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–é¢è¯•ç»éªŒçˆ¬è™«
        """
        output_dir = get_output_dir("interview")
        super().__init__(output_dir)
        self.html_file = get_html_file_path("interview")
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Returns:
            é…ç½®å­—å…¸
        """
        config_file = "search_config.json"
        if not os.path.exists(config_file):
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            # è¿”å›é»˜è®¤é…ç½®
            return DEFAULT_SEARCH_CONFIG
        
        config = load_json_data(config_file)
        if config:
            print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶æˆåŠŸ: {config_file}")
            print(f"ğŸ“‹ æœç´¢è¯æ•°é‡: {len(config.get('search_terms', []))}")
            return config
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return DEFAULT_SEARCH_CONFIG
    
    def ocr_image(self, image_path: str) -> str:
        """
        å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            OCRè¯†åˆ«ç»“æœ
        """
        ocr_tool = OCR_CONFIG["tool_path"]
        question = OCR_CONFIG["question"]
        
        if not os.path.exists(ocr_tool):
            print(f"âš ï¸ OCRå·¥å…·ä¸å­˜åœ¨: {ocr_tool}")
            return ""
        
        if not os.path.exists(image_path):
            print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return ""
        
        print(f"ğŸ” æ­£åœ¨è¯†åˆ«å›¾ç‰‡: {image_path}...")
        
        command = f"python {ocr_tool} {image_path} --question '{question}'"
        try:
            result = os.popen(command).read().strip()
            return result
        except Exception as e:
            print(f"âŒ OCRè¯†åˆ«å¤±è´¥: {e}")
            return ""
    
    def crawl_posts(self, keywords: str = "å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«", page_num: int = 1, page_size: int = 10) -> List[Dict[str, Any]]:
        """
        å®Œæ•´æŠ“å–æµç¨‹
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            åŒ…å«è¯¦æƒ…çš„å¸–å­åˆ—è¡¨
        """
        return self.search_posts(keywords, page_num, page_size)
    
    def run(self):
        """
        è¿è¡Œçˆ¬è™«ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–æœç´¢è¯
        """
        print("ğŸš€ å¯åŠ¨å°çº¢ä¹¦å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«çˆ¬è™«")
        start_time = time.time()
        
        # ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°
        search_terms = self.config.get("search_terms", ["å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«"])
        page_num = self.config.get("page_num", 1)
        page_size = self.config.get("page_size", 10)
        
        all_posts = []
        all_notes = []
        seen_note_ids = set()  # ç”¨äºå»é‡
        
        # å¯¹æ¯ä¸ªæœç´¢è¯è¿›è¡Œæœç´¢
        for keyword in search_terms:
            print(f"\nğŸ” å¤„ç†æœç´¢è¯: '{keyword}'")
            
            # 1. æœç´¢å¸–å­
            notes = self.search_posts(keyword, page_num, page_size)
            
            # å»é‡
            unique_notes = []
            for note in notes:
                note_id = note.get("note_id")
                if note_id and note_id not in seen_note_ids:
                    seen_note_ids.add(note_id)
                    unique_notes.append(note)
                    all_notes.append(note)
            
            print(f"âœ… å»é‡åå¸–å­æ•°é‡: {len(unique_notes)}")
            
            # 2. è·å–å¸–å­è¯¦æƒ…
            for i, note in enumerate(unique_notes):
                note_id = note.get("note_id")
                if not note_id:
                    continue
                    
                print(f"ğŸ“Œ å¤„ç†ç¬¬ {i+1}/{len(unique_notes)} ç¯‡å¸–å­")
                
                detail = self.get_post_detail(
                    note_id=note_id,
                    xsec_token=note.get("xsec_token", ""),
                    xsec_source=note.get("xsec_source", "pc_feed")
                )
                
                if detail:
                    post = {
                        "basic_info": note,
                        "detail": detail
                    }
                    all_posts.append(post)
                    
                    # ä¿å­˜è¯¦æƒ…
                    title = note.get("title", f"å¸–å­{i+1}")
                    clean_title = self._clean_filename(title)
                    filename = f"{len(all_posts):03d}_{clean_title}_detail.json"
                    save_json_data(post, os.path.join(self.detail_dir, filename))
                    
                    # ä¿å­˜åŸå§‹å¸–å­ä¿¡æ¯
                    post_filename = f"{len(all_posts):03d}_{clean_title}.json"
                    save_json_data(note, os.path.join(self.output_dir, post_filename))
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)
        
        if not all_posts:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å¸–å­")
            return
        
        # ä¿å­˜æ‰€æœ‰æœç´¢ç»“æœ
        all_search_result = {
            "result": {
                "code": 0,
                "msg": "success",
                "data": {
                    "notes": all_notes,
                    "total_count": len(all_notes),
                    "page_info": {
                        "current_page": page_num,
                        "page_size": page_size,
                        "has_more": False
                    }
                }
            }
        }
        save_json_data(all_search_result, os.path.join(self.output_dir, "all_search_results.json"))
        
        # 2. ç”ŸæˆHTMLç½‘é¡µ
        self.generate_html_page(all_posts, self.html_file, "å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«")
        
        end_time = time.time()
        print(f"ğŸ‰ çˆ¬è™«å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"ğŸŒ HTMLç½‘é¡µ: {self.html_file}")
        print(f"ğŸ“Š å…±æŠ“å– {len(all_posts)} ç¯‡å¸–å­")
    
    def _clean_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å
        """
        invalid_chars = ['/', '\\', ':', '*', '?', '<', '>', '|', '"']
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename


def main():
    """
    ä¸»å‡½æ•°
    """
    crawler = XhsInterviewCrawler()
    crawler.run()


if __name__ == "__main__":
    main()