#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨å¤šä¸ªå…³é”®è¯çˆ¬å–å°çº¢ä¹¦å¸–å­å¹¶ç”Ÿæˆæ€»ç»“
"""

import time
from typing import List, Dict, Any
from xhs_crawler.core.base_crawler import BaseCrawler
from xhs_crawler.core.config import get_output_dir, get_html_file_path, DEFAULT_CRAWLER_CONFIG


class MultiKeywordCrawler(BaseCrawler):
    """
    å¤šå…³é”®è¯çˆ¬è™«ï¼Œç»§æ‰¿è‡ªBaseCrawler
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å¤šå…³é”®è¯çˆ¬è™«
        """
        output_dir = get_output_dir("multi_keyword")
        super().__init__(output_dir)
        self.html_file = get_html_file_path("multi_keyword")
        self.config = DEFAULT_CRAWLER_CONFIG
    
    def search_posts_by_keyword(self, keyword: str, page_num: int = 1, page_size: int = 10) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢å°çº¢ä¹¦å¸–å­
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
            
        Returns:
            å¸–å­åˆ—è¡¨
        """
        return self.search_posts(keyword, page_num, page_size)
    
    def run(self, keywords: List[str] = None, max_pages: int = None, page_size: int = None):
        """
        è¿è¡Œå¤šå…³é”®è¯çˆ¬è™«
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            max_pages: çˆ¬å–çš„æœ€å¤§é¡µæ•°
            page_size: æ¯é¡µç»“æœæ•°é‡
        """
        print("ğŸš€ å¯åŠ¨å°çº¢ä¹¦å¤šå…³é”®è¯çˆ¬è™«")
        start_time = time.time()
        
        # ä½¿ç”¨é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å€¼
        if keywords is None:
            keywords = ["å¤§æ¨¡å‹", "é¢è¯•", "ç»éªŒåˆ†äº«"]
        if max_pages is None:
            max_pages = self.config["max_pages"]
        if page_size is None:
            page_size = self.config["page_size"]
        
        all_notes = []
        
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œå¤šé¡µçˆ¬å–
        for keyword in keywords:
            print(f"\nğŸ“Œ æ­£åœ¨çˆ¬å–å…³é”®è¯: '{keyword}'")
            for page_num in range(1, max_pages + 1):
                print(f"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {page_num}/{max_pages} é¡µ")
                notes = self.search_posts_by_keyword(keyword, page_num=page_num, page_size=page_size)
                if not notes:
                    print(f"âŒ ç¬¬ {page_num} é¡µæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å¸–å­")
                    break
                
                all_notes.extend(notes)
                print(f"âœ… ç¬¬ {page_num} é¡µçˆ¬å–åˆ° {len(notes)} ç¯‡å¸–å­")
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(self.config["sleep_time"])
        
        # å»é‡
        unique_notes = self.deduplicate_notes(all_notes)
        
        print(f"\nâœ… æ‰€æœ‰å…³é”®è¯çˆ¬å–å®Œæˆï¼Œå»é‡åå…± {len(unique_notes)} ç¯‡å¸–å­")
        
        if not unique_notes:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å¸–å­")
            return
        
        # 2. è·å–å¸–å­è¯¦æƒ…
        posts = []
        for i, note in enumerate(unique_notes):
            note_id = note.get("note_id")
            if not note_id:
                continue
                
            print(f"\nğŸ“Œ å¤„ç†ç¬¬ {i+1}/{len(unique_notes)} ç¯‡å¸–å­")
            
            detail = self.get_post_detail(
                note_id=note_id,
                xsec_token=note.get("xsec_token", ""),
                xsec_source=note.get("xsec_source", "pc_feed")
            )
            
            if detail:
                post = {
                    "basic_info": note,
                    "detail": detail
                }
                posts.append(post)
                
                # ä¿å­˜è¯¦æƒ…
                title = note.get("title", f"å¸–å­{i+1}")
                clean_title = self._clean_filename(title)
                filename = f"{i+1:03d}_{clean_title}_detail.json"
                self._save_json_data(post, f"{self.detail_dir}/{filename}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(self.config["sleep_time"])
        
        if not posts:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•å¸–å­è¯¦æƒ…")
            return
        
        # 3. ç”ŸæˆHTMLç½‘é¡µ
        self.generate_html_page(posts, self.html_file, "å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº« - å…¨é‡")
        
        end_time = time.time()
        print(f"ğŸ‰ çˆ¬è™«å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"ğŸŒ HTMLç½‘é¡µ: {self.html_file}")
    
    def _clean_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å
        """
        invalid_chars = ['/', '\\', ':', '*', '?', '<', '>', '|', '"']
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename
    
    def _save_json_data(self, data, file_path):
        """
        ä¿å­˜JSONæ•°æ®
        """
        from xhs_crawler.core.mcp_utils import save_json_data
        return save_json_data(data, file_path)


def main():
    """
    ä¸»å‡½æ•°
    """
    crawler = MultiKeywordCrawler()
    crawler.run()


if __name__ == "__main__":
    main()