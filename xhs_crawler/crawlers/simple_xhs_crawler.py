#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„å°çº¢ä¹¦çˆ¬è™«è„šæœ¬ï¼Œç”¨äºæŠ“å–å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«å¸–å­
"""

import time
from xhs_crawler.core.base_crawler import BaseCrawler
from xhs_crawler.core.config import get_output_dir, get_html_file_path


class SimpleXhsCrawler(BaseCrawler):
    """
    ç®€å•çš„å°çº¢ä¹¦çˆ¬è™«ï¼Œç»§æ‰¿è‡ªBaseCrawler
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç®€å•çˆ¬è™«
        """
        output_dir = get_output_dir("simple")
        super().__init__(output_dir)
        self.html_file = get_html_file_path("simple")
    
    def run(self, keywords: str = "å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«", page_num: int = 1, page_size: int = 10):
        """
        è¿è¡Œçˆ¬è™«
        
        Args:
            keywords: æœç´¢å…³é”®è¯
            page_num: é¡µç 
            page_size: æ¯é¡µæ•°é‡
        """
        print("ğŸš€ å¯åŠ¨å°çº¢ä¹¦å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«çˆ¬è™«")
        start_time = time.time()
        
        # 1. æœç´¢å¸–å­
        notes = self.search_posts(keywords, page_num, page_size)
        if not notes:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å¸–å­")
            return
        
        # 2. è·å–å¸–å­è¯¦æƒ…
        posts = []
        for i, note in enumerate(notes):
            note_id = note.get("note_id")
            if not note_id:
                continue
                
            print(f"\nğŸ“Œ å¤„ç†ç¬¬ {i+1}/{len(notes)} ç¯‡å¸–å­")
            
            detail = self.get_post_detail(
                note_id=note_id,
                xsec_token=note.get("xsec_token", ""),
                xsec_source=note.get("xsec_source", "pc_feed")
            )
            
            if detail:
                post = {
                    "basic_info": note,
                    "detail": detail
                }
                posts.append(post)
                
                # ä¿å­˜å¸–å­å’Œè¯¦æƒ…
                title = note.get("title", f"å¸–å­{i+1}")
                clean_title = self._clean_filename(title)
                filename = f"{i+1:03d}_{clean_title}_detail.json"
                self._save_json_data(post, f"{self.detail_dir}/{filename}")
                
                # ä¿å­˜åŸå§‹å¸–å­
                post_filename = f"{i+1:03d}_{clean_title}.json"
                self._save_json_data(note, f"{self.output_dir}/{post_filename}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        if not posts:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•å¸–å­è¯¦æƒ…")
            return
        
        # 3. ç”ŸæˆHTMLç½‘é¡µ
        self.generate_html_page(posts, self.html_file, "å¤§æ¨¡å‹é¢è¯•ç»éªŒåˆ†äº«")
        
        end_time = time.time()
        print(f"ğŸ‰ çˆ¬è™«å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"ğŸŒ HTMLç½‘é¡µ: {self.html_file}")
    
    def _clean_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å
        """
        invalid_chars = ['/', '\\', ':', '*', '?', '<', '>', '|', '"']
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename
    
    def _save_json_data(self, data, file_path):
        """
        ä¿å­˜JSONæ•°æ®
        """
        from xhs_crawler.core.mcp_utils import save_json_data
        return save_json_data(data, file_path)


def main():
    """
    ä¸»å‡½æ•°
    """
    crawler = SimpleXhsCrawler()
    crawler.run(keywords="å¤§æ¨¡å‹é¢è¯• ç»éªŒåˆ†äº«", page_num=1, page_size=10)


if __name__ == "__main__":
    main()