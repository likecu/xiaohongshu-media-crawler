#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é›†æˆçš„ AI/ML åŠŸèƒ½
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xhs_crawler.core.base_crawler import BaseCrawler


def test_ai_integration():
    """
    æµ‹è¯• AI åŠŸèƒ½é›†æˆ
    
    Returns:
        æµ‹è¯•ç»“æœ
    """
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• AI/ML åŠŸèƒ½é›†æˆ")
    print("=" * 60)
    
    crawler = BaseCrawler(output_dir="test_output")
    
    # æµ‹è¯• 1: éªŒè¯ AIUtils å®ä¾‹æ˜¯å¦åˆ›å»º
    print("\nğŸ“‹ æµ‹è¯• 1: éªŒè¯ AIUtils å®ä¾‹...")
    if hasattr(crawler, 'ai_utils'):
        print("âœ… AIUtils å®ä¾‹å·²åˆ›å»º")
        print(f"   - ç±»å‹: {type(crawler.ai_utils).__name__}")
    else:
        print("âŒ AIUtils å®ä¾‹æœªåˆ›å»º")
        return False
    
    # æµ‹è¯• 2: éªŒè¯ TF-IDF å‘é‡åŒ–å™¨
    print("\nğŸ“‹ æµ‹è¯• 2: éªŒè¯ TF-IDF å‘é‡åŒ–å™¨...")
    if hasattr(crawler.ai_utils, 'tfidf_vectorizer'):
        print("âœ… TF-IDF å‘é‡åŒ–å™¨å·²åˆå§‹åŒ–")
    else:
        print("âŒ TF-IDF å‘é‡åŒ–å™¨æœªåˆå§‹åŒ–")
        return False
    
    # æµ‹è¯• 3: æµ‹è¯•å¢å¼ºå†…å®¹æ€»ç»“
    print("\nğŸ“‹ æµ‹è¯• 3: æµ‹è¯•å¢å¼ºå†…å®¹æ€»ç»“...")
    test_content = "è¿™æ˜¯ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½é¢è¯•ç»éªŒçš„åˆ†äº«æ–‡ç« ã€‚"
    test_title = "AI é¢è¯•ç»éªŒæ€»ç»“"
    test_images = []
    
    summary = crawler.ai_utils.summarize_content_enhanced(
        content=test_content,
        title=test_title,
        images=test_images
    )
    
    if summary:
        print("âœ… å¢å¼ºå†…å®¹æ€»ç»“ç”ŸæˆæˆåŠŸ")
        print(f"   - æ€»ç»“: {summary.get('summary', '')[:100]}...")
        print(f"   - æƒ…æ„Ÿ: {summary.get('sentiment', '')}")
        print(f"   - å…³é”®ä¿¡æ¯: {summary.get('key_points', [])}")
    else:
        print("âŒ å¢å¼ºå†…å®¹æ€»ç»“ç”Ÿæˆå¤±è´¥")
        return False
    
    # æµ‹è¯• 4: æµ‹è¯•å†…å®¹ç´¢å¼•æ„å»º
    print("\nğŸ“‹ æµ‹è¯• 4: æµ‹è¯•å†…å®¹ç´¢å¼•æ„å»º...")
    test_posts = [
        {
            "basic_info": {"note_id": "1", "title": "AI é¢è¯•æŠ€å·§ä¸ç»éªŒåˆ†äº«"},
            "detail": {"desc": "æœ¬æ–‡åˆ†äº«äº†äººå·¥æ™ºèƒ½é¢†åŸŸé¢è¯•çš„å®ç”¨æŠ€å·§ï¼ŒåŒ…æ‹¬æŠ€æœ¯é¢è¯•å‡†å¤‡ã€é¡¹ç›®ç»éªŒé˜è¿°ã€ç®—æ³•é—®é¢˜è§£ç­”ç­–ç•¥ç­‰å…³é”®è¦ç‚¹ã€‚å»ºè®®é¢è¯•è€…æå‰å‡†å¤‡æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯†ï¼Œç†Ÿç»ƒæŒæ¡å¸¸è§ç®—æ³•å¦‚çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€å·ç§¯ç¥ç»ç½‘ç»œç­‰ã€‚åŒæ—¶è¦å‡†å¤‡å¥½é¡¹ç›®ç»éªŒä»‹ç»ï¼Œèƒ½å¤Ÿæ¸…æ™°é˜è¿°é¡¹ç›®èƒŒæ™¯ã€æŠ€æœ¯æ–¹æ¡ˆã€é‡åˆ°çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚é¢è¯•æ—¶ä¿æŒè‡ªä¿¡ï¼Œæ¡ç†æ¸…æ™°ï¼Œå±•ç°è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›å’Œå­¦ä¹ èƒ½åŠ›ã€‚"}
        },
        {
            "basic_info": {"note_id": "2", "title": "æœºå™¨å­¦ä¹ å…¥é—¨å®Œå…¨æŒ‡å—"},
            "detail": {"desc": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œæœ¬æ–‡ç³»ç»Ÿä»‹ç»äº†æœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†å’Œå…¥é—¨è·¯å¾„ã€‚é¦–å…ˆéœ€è¦æŒæ¡æ•°å­¦åŸºç¡€ï¼ŒåŒ…æ‹¬çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ç­‰ã€‚ç„¶åå­¦ä¹ Pythonç¼–ç¨‹è¯­è¨€å’Œå¸¸ç”¨åº“å¦‚NumPyã€Pandasã€Scikit-learnç­‰ã€‚å…¥é—¨é˜¶æ®µå»ºè®®ä»ç›‘ç£å­¦ä¹ å¼€å§‹ï¼Œå­¦ä¹ åˆ†ç±»å’Œå›å½’é—®é¢˜ï¼Œå¸¸ç”¨ç®—æ³•åŒ…æ‹¬é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºã€éšæœºæ£®æ—ç­‰ã€‚å®è·µä¸­å¯ä»¥ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¦‚Irisã€MNISTç­‰è¿›è¡Œç»ƒä¹ ã€‚å»ºè®®è¾¹å­¦è¾¹åšï¼Œé€šè¿‡å®é™…é¡¹ç›®å·©å›ºç†è®ºçŸ¥è¯†ã€‚"}
        },
        {
            "basic_info": {"note_id": "3", "title": "æ·±åº¦å­¦ä¹ å®æˆ˜é¡¹ç›®æ¡ˆä¾‹åˆ†æ"},
            "detail": {"desc": "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚æœ¬æ–‡é€šè¿‡å®é™…æ¡ˆä¾‹ä»‹ç»æ·±åº¦å­¦ä¹ é¡¹ç›®çš„å®Œæ•´æµç¨‹ã€‚ä»¥å›¾åƒåˆ†ç±»é¡¹ç›®ä¸ºä¾‹ï¼Œé¦–å…ˆè¿›è¡Œæ•°æ®æ”¶é›†å’Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºã€å½’ä¸€åŒ–ç­‰ã€‚ç„¶åé€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„ï¼Œå¦‚ResNetã€VGGç­‰ï¼Œåˆ©ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯åŠ é€Ÿè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼Œä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆå°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæä¾›APIæœåŠ¡ã€‚å®æˆ˜ä¸­è¦æ³¨æ„è®¡ç®—èµ„æºåˆ†é…ã€æ¨¡å‹å‹ç¼©ä¼˜åŒ–ã€æ¨ç†é€Ÿåº¦æå‡ç­‰é—®é¢˜ã€‚"}
        }
    ]
    
    try:
        crawler.ai_utils.build_content_index(test_posts)
        print("âœ… å†…å®¹ç´¢å¼•æ„å»ºæˆåŠŸ")
        print(f"   - ç´¢å¼•å¸–å­æ•°: {len(test_posts)}")
        print(f"   - å‘é‡ç»´åº¦: {crawler.ai_utils.post_vectors.shape if crawler.ai_utils.post_vectors is not None else 'N/A'}")
    except Exception as e:
        print(f"âŒ å†…å®¹ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• 5: æµ‹è¯•ç›¸ä¼¼å¸–å­æœç´¢
    print("\nğŸ“‹ æµ‹è¯• 5: æµ‹è¯•ç›¸ä¼¼å¸–å­æœç´¢...")
    try:
        similar = crawler.ai_utils.search_similar_posts("AIé¢è¯•", top_k=2)
        print(f"âœ… ç›¸ä¼¼å¸–å­æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(similar)} ç¯‡ç›¸å…³å¸–å­")
        for i, post in enumerate(similar):
            print(f"   - {i+1}. {post.get('basic_info', {}).get('title', 'Unknown')} (ç›¸ä¼¼åº¦: {post.get('similarity', 0):.4f})")
    except Exception as e:
        print(f"âŒ ç›¸ä¼¼å¸–å­æœç´¢å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• 6: æµ‹è¯•å¸–å­æ¨è
    print("\nğŸ“‹ æµ‹è¯• 6: æµ‹è¯•å¸–å­æ¨è...")
    try:
        recommendations = crawler.ai_utils.recommend_posts("1", top_k=2)
        print(f"âœ… å¸–å­æ¨èæˆåŠŸï¼Œæ¨è {len(recommendations)} ç¯‡å¸–å­")
        for i, post in enumerate(recommendations):
            print(f"   - {i+1}. {post.get('basic_info', {}).get('title', 'Unknown')} (ç›¸ä¼¼åº¦: {post.get('similarity', 0):.4f})")
    except Exception as e:
        print(f"âŒ å¸–å­æ¨èå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• 7: æµ‹è¯•è¶‹åŠ¿åˆ†æ
    print("\nğŸ“‹ æµ‹è¯• 7: æµ‹è¯•è¶‹åŠ¿åˆ†æ...")
    try:
        trends = crawler.ai_utils.analyze_trends(test_posts, time_window="month")
        print("âœ… è¶‹åŠ¿åˆ†ææˆåŠŸ")
        print(f"   - å…³é”®è¯: {trends.get('keywords', [])}")
        print(f"   - åˆ†ç±»: {trends.get('categories', {})}")
    except Exception as e:
        print(f"âŒ è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
        return False
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰ AI/ML åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    success = test_ai_integration()
    sys.exit(0 if success else 1)
